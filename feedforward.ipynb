{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiU6rcz91tKntCbnI5aEuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helenjoy/CV_Units/blob/main/feedforward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying hand-written digits(MNIST) using PyTorch**\n",
        "\n",
        "We will use the famous MNIST Handwritten Digits Databases as our training dataset.It consists of 28px by 28px grayscale images of handwritten disgits(0 - 9), along with labels for each image indicating which digit it represents. MNIST stands for Modified National Institute of Standards and Technology."
      ],
      "metadata": {
        "id": "FJDtjH1l1qgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lf4igxx1nIh",
        "outputId": "003d9572-880e-410e-bb4e-daeb11372803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'mnist-in-csv' dataset.\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"oddrationale/mnist-in-csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "import torch\n",
        "import torchvision ## Contains some utilities for working with the image data\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "X483oAgw3Evz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MNIST(root = 'data/', download = True)\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hscEenv3Qy5",
        "outputId": "cfdd7c9d-edf8-459d-e611-9d8947b45b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 46.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.18MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.7MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.89MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = dataset[10]\n",
        "plt.imshow(image, cmap = 'gray')\n",
        "print('Label:', label)"
      ],
      "metadata": {
        "id": "o14WzCOE3WPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Transform images to tensors and normalize\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load training dataset\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4CsKsB0IFrq",
        "outputId": "c5bce81e-c3e2-46b2-a43d-a0121c4b63ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 14.2MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 277kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.02MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 7.64MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These images are small in size, and recognizing the digits can sometimes be hard. PyTorch doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset.\n",
        "\n",
        "PyTorch datasets allow us to specify one or more transformation function which are applied to the images as they are loaded.\n",
        "\n",
        " torchvision.transforms contains many such predefined functions and we will use ToTensor transform to convert images into Pytorch tensors."
      ],
      "metadata": {
        "id": "5FkPGaTx3gAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MNIST dataset(images and labels)\n",
        "mnist_dataset = MNIST(root = 'data/', train = True, transform = transforms.ToTensor())\n",
        "print(mnist_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfQ9zbBx3mu8",
        "outputId": "0a794cab-f269-43e2-fe96-8e0308946653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_tensor, label = mnist_dataset[0]\n",
        "print(image_tensor.shape, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM1p2sTm35Ur",
        "outputId": "d8f5c814-742f-4642-d4e6-91daaba3fb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28]) 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The image is now convert to a 28 X 28 tensor.The first dimension is used to keep track of the color channels. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in that case the color channels would be 3(Red, Green, Blue).**"
      ],
      "metadata": {
        "id": "N8K6h86d39my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_tensor[:,10:15,10:15])\n",
        "print(torch.max(image_tensor), torch.min(image_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujnuz2uj4C1f",
        "outputId": "36e81025-68a7-413c-a069-2c736676b852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
            "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
            "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
            "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
            "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
            "tensor(1.) tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The values range from 0 to 1, with 0 representing black, 1 white and the values between different shades of grey. We can also plot the tensor as an image using lt.imshow**"
      ],
      "metadata": {
        "id": "K8-1-wvT4ZYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data = random_split(mnist_dataset, [50000, 10000])\n",
        "## Print the length of train and validation datasets\n",
        "print(\"length of Train Datasets: \", len(train_data))\n",
        "print(\"length of Validation Datasets: \", len(validation_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPPTHc7b4hsR",
        "outputId": "e0258640-75cb-4ddd-d171-e546173c1d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of Train Datasets:  50000\n",
            "length of Validation Datasets:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will use DataLoaders to help us load the data in batches. We will use a batch size of 128. We will set shuffle = True for the training dataloader, so that the batches generated in each epoch are different, and this randomization helps in generalizing and speed up the process.\n",
        "\n",
        "Since Validation dataloader is used only for evaluating the model, there is no need to shuffle the images."
      ],
      "metadata": {
        "id": "l3Jeh4PZ5rMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28 * 28\n",
        "num_classes = 10\n",
        "\n",
        "## Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "print(model.weight.shape)\n",
        "print(model.weight)\n",
        "print(model.bias.shape)\n",
        "print(model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8siaMD8W5q3R",
        "outputId": "43a1d2a6-f241-4b08-86db-1de6454d724f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 784])\n",
            "Parameter containing:\n",
            "tensor([[-0.0015, -0.0204,  0.0230,  ..., -0.0296,  0.0121, -0.0225],\n",
            "        [ 0.0020,  0.0211, -0.0060,  ...,  0.0161, -0.0141, -0.0162],\n",
            "        [-0.0277, -0.0293, -0.0310,  ...,  0.0051, -0.0126,  0.0013],\n",
            "        ...,\n",
            "        [ 0.0201,  0.0112, -0.0169,  ...,  0.0306, -0.0232, -0.0105],\n",
            "        [-0.0070, -0.0277, -0.0059,  ..., -0.0266,  0.0245, -0.0270],\n",
            "        [ 0.0299,  0.0215,  0.0022,  ...,  0.0043,  0.0311, -0.0294]],\n",
            "       requires_grad=True)\n",
            "torch.Size([10])\n",
            "Parameter containing:\n",
            "tensor([ 0.0075, -0.0331,  0.0140,  0.0309, -0.0241, -0.0245, -0.0299,  0.0185,\n",
            "         0.0278, -0.0005], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression model is identical to a linear regression model i.e, there are weights and bias matrices, and the output is obtained using simple matrix operations(pred = x@ w.t() + b).\n",
        "\n",
        "We can use nn.Linear to create the model instead of defining and initializing the matrices manually.\n",
        "\n",
        "Since nn.Linear expects the each training example to a vector, each 1 X 28 X 28 image tensor needs to be flattened out into a vector of size 784(28 X 28), before being passed into the model.\n",
        "\n",
        "The output for each image is vector of size 10, with each element of the vector signifying the probability a particular target label(i.e 0 to 9). The predicted label for an image is simply the one with the highest probability."
      ],
      "metadata": {
        "id": "hdQmKjl454lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        print(xb)\n",
        "        out = self.linear(xb)\n",
        "        print(out)\n",
        "        return(out)\n",
        "\n",
        "model = MnistModel()\n",
        "print(model.linear.weight.shape, model.linear.bias.shape)\n",
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx0JUWo354Hn",
        "outputId": "2fdd0a3a-5608-4711-f7cb-1416f50e6d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 784]) torch.Size([10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0258,  0.0344,  0.0134,  ..., -0.0244,  0.0040, -0.0191],\n",
              "         [ 0.0231, -0.0308, -0.0225,  ...,  0.0157,  0.0063, -0.0032],\n",
              "         [-0.0261,  0.0217,  0.0163,  ...,  0.0068, -0.0170, -0.0334],\n",
              "         ...,\n",
              "         [-0.0020,  0.0176, -0.0256,  ...,  0.0315,  0.0312, -0.0046],\n",
              "         [-0.0207,  0.0212,  0.0039,  ...,  0.0238,  0.0223,  0.0074],\n",
              "         [ 0.0140,  0.0151,  0.0255,  ..., -0.0247,  0.0210,  0.0050]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0230,  0.0198,  0.0294, -0.0220, -0.0079, -0.0308,  0.0140,  0.0172,\n",
              "          0.0244, -0.0102], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the init constructor method, we instantiate the weights and biases using nn.Linear. Inside the forward method, which is invoked when we pass a batch of inputs to the model, we flatten out the input tensor, and then pass it into self.linear.\n",
        "\n",
        "xb.reshape(-1, 28 * 28) indicates to PyTorch that we want a view of the xb tensor with two dimensions, where the length along the 2nd dimension is 28 * 28(i.e 784). One argument to .reshape can be set to -1(in this case the first dimension), to let PyTorch figure it out automatically based on the shape of the original tensor.\n",
        "\n",
        "Note that the model no longer has .weight and .bias attributes(as they are now inside the .linear attribute),but it does have a .parameters method which returns a list containg the weights and bias, and can be used by a PyTorch optimizer.\n"
      ],
      "metadata": {
        "id": "F7UNGeFp6pei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    outputs = model(images)\n",
        "    break\n",
        "\n",
        "print('outputs shape: ', outputs.shape)\n",
        "print('Sample outputs: \\n', outputs[:2].data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZnkIvL96n7j",
        "outputId": "9cb4650f-c33b-43a7-91e3-ae992919b511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0824, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
            "tensor([[-1.5296e-01,  3.8717e-02, -1.3359e-02, -1.7384e-01, -8.1495e-02,\n",
            "          3.2846e-01, -1.8180e-01,  1.2243e-02,  1.0970e-01,  8.6983e-02],\n",
            "        [-4.7125e-01,  6.0984e-01, -8.0295e-01, -2.2474e-01,  3.9914e-01,\n",
            "         -5.0947e-03, -9.5121e-01, -2.9335e-01,  2.6338e-01, -1.7028e-01],\n",
            "        [-2.7910e-01,  3.4566e-01, -5.7364e-01, -2.6045e-01,  3.8741e-01,\n",
            "          8.9853e-02, -6.4902e-01, -1.1373e-01,  1.2724e-01,  2.1690e-03],\n",
            "        [-5.8175e-01,  5.6425e-01, -1.0862e+00, -2.1219e-01,  3.3368e-01,\n",
            "          2.4260e-01, -1.0067e+00, -4.0404e-01,  1.8727e-01, -2.8301e-01],\n",
            "        [-1.9318e-01,  1.7406e-01, -3.7271e-01,  1.1723e-01,  4.4553e-02,\n",
            "          3.3199e-01, -6.4536e-02,  6.9047e-02,  1.0310e-01, -1.4903e-01],\n",
            "        [-4.5774e-01,  4.9280e-01, -3.4769e-01, -1.9672e-01,  3.5534e-01,\n",
            "          1.2948e-01, -2.1457e-01, -1.8788e-01,  9.8467e-02, -3.0827e-02],\n",
            "        [ 5.8261e-02,  9.2350e-02, -1.9705e-01,  1.1402e-01,  7.3543e-02,\n",
            "          7.0600e-02, -2.1729e-01, -1.5766e-01,  5.5084e-03, -1.4376e-01],\n",
            "        [-8.6877e-02,  2.8331e-01,  3.3351e-02, -1.7741e-01,  2.4985e-01,\n",
            "         -3.9429e-02, -2.8500e-01, -7.4649e-02,  1.9942e-01, -7.6777e-02],\n",
            "        [-3.2930e-01,  4.4087e-01, -5.5396e-01, -1.3096e-01,  3.8701e-01,\n",
            "          2.5870e-02, -4.8896e-01, -1.5632e-01,  2.1921e-01, -1.0752e-01],\n",
            "        [-3.0837e-01,  6.1546e-01, -5.4406e-01, -1.9206e-01,  3.1859e-01,\n",
            "          1.9842e-02, -5.4436e-01, -1.7070e-01,  2.6454e-01, -4.1889e-01],\n",
            "        [-5.1536e-01,  5.9525e-01, -6.8633e-01, -2.1084e-01,  2.3248e-01,\n",
            "          2.1377e-01, -6.5640e-01, -2.5025e-01,  2.6237e-01, -2.5047e-01],\n",
            "        [-6.4059e-02,  4.5146e-01, -6.5488e-01, -2.1715e-01,  1.1958e-01,\n",
            "          1.0607e-01, -5.5768e-01, -3.3861e-01,  2.2126e-01, -1.0266e-01],\n",
            "        [-3.4287e-01,  2.2426e-01, -3.4987e-01, -4.2450e-02,  2.0086e-01,\n",
            "          1.6278e-01, -1.6520e-01, -2.2863e-01,  1.5011e-01, -2.8065e-01],\n",
            "        [-2.5049e-01,  5.5891e-01, -4.8523e-01, -2.5859e-01,  2.3273e-01,\n",
            "          2.8347e-01, -6.8174e-01, -2.2840e-01,  2.1841e-01, -1.7755e-01],\n",
            "        [-3.5674e-01,  5.9020e-01, -4.5070e-01, -2.3267e-01,  3.2793e-01,\n",
            "         -4.0441e-02, -4.7785e-01, -2.3438e-01, -8.2664e-02, -1.2228e-01],\n",
            "        [-3.7223e-01,  4.1352e-01, -4.0646e-01, -2.2916e-01,  3.2202e-01,\n",
            "          3.0383e-01, -8.8369e-02, -1.5774e-01, -3.2768e-02, -1.2875e-01],\n",
            "        [-3.6853e-01,  4.8378e-01, -4.6849e-01,  7.3089e-02,  3.6322e-01,\n",
            "          2.0079e-01, -2.4448e-01, -3.5364e-01,  1.8176e-01, -2.4216e-01],\n",
            "        [-2.4981e-01,  1.7984e-01, -3.3830e-01, -1.3491e-01,  3.2352e-01,\n",
            "         -1.7148e-01, -3.3691e-01, -2.9598e-02, -2.3711e-01, -1.3097e-01],\n",
            "        [-1.5340e-01,  2.8732e-01, -7.9213e-02, -2.5256e-02,  5.9357e-02,\n",
            "          2.8458e-02, -1.7081e-01, -3.2192e-01,  8.5437e-02, -2.7229e-01],\n",
            "        [-5.4768e-01,  9.2290e-01, -9.3602e-01, -2.5522e-01,  3.0957e-01,\n",
            "          1.8709e-01, -8.5610e-01, -3.5802e-01,  2.8684e-01,  6.6029e-02],\n",
            "        [-2.9995e-02,  2.7110e-01, -3.5802e-01,  1.1905e-03, -1.2421e-01,\n",
            "          2.1750e-01,  1.5347e-01,  1.2638e-01, -1.3977e-01,  5.7371e-02],\n",
            "        [ 6.0963e-03,  2.4477e-01, -2.3512e-01, -1.5280e-01,  1.7218e-01,\n",
            "         -2.1033e-02, -5.5910e-01, -1.2486e-01,  1.9392e-01,  2.6929e-02],\n",
            "        [-3.7305e-01,  3.0014e-01, -6.9535e-01,  9.3506e-02,  3.2002e-01,\n",
            "          4.2710e-01, -2.1727e-01, -1.1822e-01,  1.7028e-01, -2.6806e-01],\n",
            "        [-3.9977e-01,  4.8550e-01, -2.6214e-01, -2.2569e-01,  2.4731e-01,\n",
            "         -4.1931e-02, -3.8027e-01, -5.5028e-02, -1.5894e-01, -1.8356e-01],\n",
            "        [-2.6847e-01,  2.5859e-01, -2.0253e-01, -5.7997e-02,  1.5350e-01,\n",
            "         -1.0583e-01, -1.7350e-01,  1.6117e-02,  6.3864e-02, -2.3395e-01],\n",
            "        [-5.9864e-01,  4.8453e-01, -9.2084e-01, -1.0155e-01,  5.0056e-01,\n",
            "          1.7838e-01, -9.1540e-01, -2.4605e-01,  2.7668e-01, -1.5497e-01],\n",
            "        [-1.9330e-01,  3.4339e-01, -2.4464e-01,  4.3576e-02,  2.9611e-01,\n",
            "          2.8644e-01, -2.1191e-01,  3.6098e-01,  1.5372e-01,  5.0160e-02],\n",
            "        [-2.6347e-01,  3.2947e-01, -3.8539e-01, -1.9806e-01,  5.6722e-01,\n",
            "         -9.7382e-02, -4.0303e-01, -6.5741e-02, -3.6307e-01, -9.3666e-02],\n",
            "        [-3.3159e-01,  5.1605e-01, -4.7124e-01,  3.4186e-02,  5.1083e-01,\n",
            "          2.6561e-01, -2.7568e-01, -2.8202e-01,  8.4473e-02, -2.5127e-01],\n",
            "        [-3.9808e-02,  1.2029e-01, -4.0684e-01,  1.0802e-01,  1.2215e-01,\n",
            "          1.0068e-01, -7.7963e-02, -6.0704e-03,  3.4470e-01, -2.5173e-01],\n",
            "        [-2.8119e-01,  4.8191e-01, -7.1091e-01, -2.8039e-01,  1.5745e-01,\n",
            "          1.8156e-01, -7.8982e-01, -5.1998e-01,  3.5641e-01, -1.6053e-01],\n",
            "        [-4.8473e-01,  4.9939e-01, -7.3632e-01, -1.6073e-01,  3.1380e-01,\n",
            "          1.3697e-01, -8.5798e-01, -2.4986e-01,  2.2087e-01, -1.8898e-01],\n",
            "        [-1.6080e-01,  3.6709e-01, -2.2959e-01, -1.4954e-01,  2.4351e-01,\n",
            "          4.9166e-01, -4.2605e-01,  2.0463e-02,  3.5124e-01,  9.9402e-02],\n",
            "        [-2.9715e-01,  5.2553e-01, -3.6785e-01, -5.5941e-02,  2.7590e-01,\n",
            "          4.7883e-02, -2.1949e-01, -1.4345e-01,  5.0660e-02, -9.3148e-03],\n",
            "        [-2.8061e-01,  2.8043e-01, -1.5703e-01, -1.3327e-01,  2.4964e-01,\n",
            "         -1.0527e-01, -3.3146e-01, -8.2107e-02, -1.3571e-01,  7.5636e-03],\n",
            "        [-6.1117e-01,  8.0478e-01, -6.8861e-01, -2.0846e-01,  3.6279e-01,\n",
            "          1.8372e-01, -6.0960e-01, -3.1926e-01,  4.5740e-01, -1.9465e-01],\n",
            "        [-1.9542e-01,  4.9752e-01, -4.4398e-01,  8.0387e-02, -8.2675e-02,\n",
            "          5.0100e-01, -2.7525e-02, -7.9777e-02, -4.1751e-02, -1.0536e-01],\n",
            "        [-1.8642e-01,  2.4797e-01, -9.3654e-02,  2.1061e-02,  2.8497e-01,\n",
            "         -1.5536e-01, -2.3772e-01, -3.2471e-02, -1.2783e-01,  1.1490e-01],\n",
            "        [ 2.2840e-02,  3.3927e-01, -3.3125e-01,  7.5165e-02,  2.6183e-01,\n",
            "         -1.3968e-01, -1.5245e-01, -1.3818e-01,  4.9678e-01, -3.9032e-01],\n",
            "        [-4.5832e-01,  5.6253e-01, -5.0428e-01, -2.8430e-01,  3.4843e-01,\n",
            "          1.6976e-01, -4.4698e-01, -2.4322e-01,  1.0247e-01, -3.6376e-01],\n",
            "        [-3.4175e-01,  6.6182e-01, -1.0700e+00, -4.6734e-01,  5.7896e-01,\n",
            "          2.1122e-01, -5.6077e-01, -3.5780e-01,  1.7951e-01, -2.6914e-01],\n",
            "        [ 1.2564e-02,  9.5951e-02, -1.2473e-01,  7.4912e-02,  1.0577e-02,\n",
            "          1.1980e-01,  1.9006e-02,  7.2191e-02, -8.0588e-02, -1.0887e-01],\n",
            "        [-3.8423e-01,  6.1291e-01, -7.3539e-01, -1.0850e-01,  3.2633e-01,\n",
            "          2.6275e-01, -6.0370e-01, -2.6291e-01,  3.6085e-01, -2.8484e-01],\n",
            "        [-4.2887e-01,  8.8847e-01, -5.4351e-01, -2.3157e-01,  4.7988e-01,\n",
            "          2.2891e-01, -5.0630e-01, -3.4310e-01,  2.6314e-01, -4.1924e-02],\n",
            "        [-2.3328e-01,  1.6708e-01, -2.5330e-01, -2.0326e-01,  3.9169e-01,\n",
            "          7.9823e-04, -2.9576e-01,  3.9307e-02, -2.9758e-01, -5.4532e-02],\n",
            "        [-3.4419e-01,  4.1927e-01, -4.4903e-01, -1.3626e-01,  3.3228e-01,\n",
            "          9.5085e-02, -4.8042e-01, -1.4928e-01,  2.5497e-01, -1.4374e-01],\n",
            "        [ 1.4805e-01,  4.8590e-02, -4.3119e-02,  1.6047e-01, -9.5281e-02,\n",
            "          3.1303e-01, -3.9940e-02, -2.7693e-02,  8.5211e-02, -1.3648e-02],\n",
            "        [-3.1993e-01,  4.3371e-01, -4.3482e-01, -8.1001e-02,  1.3433e-01,\n",
            "          1.5968e-01, -4.2009e-01, -2.0971e-01,  7.2463e-02, -9.7721e-02],\n",
            "        [-3.0901e-01,  1.9211e-01, -4.9156e-01,  1.0283e-01,  1.3788e-01,\n",
            "          3.3768e-01, -7.7102e-02,  1.9430e-01, -7.6736e-02, -1.0280e-01],\n",
            "        [-2.7240e-01,  2.3807e-01, -2.7764e-01,  1.2808e-01,  8.8898e-02,\n",
            "          2.2339e-01,  2.4385e-02,  1.1749e-01, -3.3363e-02, -1.1518e-01],\n",
            "        [-1.7851e-01,  1.0548e-01, -1.7829e-01,  3.8877e-02, -5.9186e-02,\n",
            "          9.8931e-02, -7.7452e-03,  1.0239e-02,  1.2150e-01, -5.0289e-02],\n",
            "        [-3.4249e-01,  2.3950e-01, -5.2449e-01,  3.7846e-02,  1.9578e-01,\n",
            "          2.1700e-01, -1.3434e-01, -1.9202e-01,  1.6670e-01, -2.2516e-01],\n",
            "        [-1.6176e-01,  1.2865e-01, -2.2629e-01, -1.3954e-01,  3.2853e-02,\n",
            "         -9.9867e-02, -2.7270e-01, -1.3032e-01,  2.3829e-01, -3.5944e-02],\n",
            "        [-1.7512e-01,  2.0121e-01, -1.5067e-01, -1.3640e-01,  2.3154e-01,\n",
            "         -4.9006e-02, -2.3014e-01, -1.2456e-01,  1.6107e-01, -1.1951e-01],\n",
            "        [-1.9845e-01,  2.9648e-01, -8.9806e-02, -1.8143e-01,  2.5211e-01,\n",
            "         -3.6904e-02, -1.6355e-01, -9.4247e-02, -4.2690e-02, -1.3357e-02],\n",
            "        [-3.4813e-01,  6.6927e-01, -6.9555e-01, -1.9254e-01,  3.1147e-01,\n",
            "          1.3660e-01, -5.8322e-01, -3.4875e-01,  4.1519e-01, -6.0425e-02],\n",
            "        [-3.0633e-01,  5.7761e-01, -3.0021e-01, -1.9199e-01,  1.7976e-01,\n",
            "          2.6463e-03, -4.7242e-01, -1.4392e-01, -2.4827e-02, -5.4343e-02],\n",
            "        [-2.1952e-01,  4.1221e-01, -4.8328e-01, -2.2714e-01,  4.2165e-01,\n",
            "          6.8773e-02, -6.0033e-01, -5.9862e-02,  3.2896e-02, -3.0462e-01],\n",
            "        [-3.2822e-01,  2.1880e-01, -4.5026e-01, -1.0391e-01,  1.7044e-01,\n",
            "          3.0691e-02, -5.5610e-01, -3.8164e-01,  6.7787e-02,  1.0397e-01],\n",
            "        [-6.3172e-01,  6.3838e-01, -8.6688e-01, -1.2704e-01,  2.6454e-01,\n",
            "          2.8578e-01, -8.7652e-01, -2.3728e-01,  2.0893e-01, -2.7657e-01],\n",
            "        [-1.6268e-01,  1.6949e-01, -4.8039e-01,  2.7951e-01,  3.0251e-01,\n",
            "          3.3490e-01, -2.4291e-01,  5.3372e-02,  2.6504e-01, -2.4746e-01],\n",
            "        [-3.5787e-01,  3.0898e-01, -2.1092e-01, -1.1800e-01,  3.6481e-01,\n",
            "         -1.6494e-02, -3.1446e-01, -2.1943e-02, -1.4862e-01, -1.5340e-01],\n",
            "        [-6.0119e-01,  7.9429e-01, -5.3715e-01, -5.9458e-02,  1.8608e-01,\n",
            "          2.7527e-01, -5.8411e-01, -3.5444e-01,  2.1872e-01,  1.0718e-01],\n",
            "        [-1.8723e-01,  2.3699e-01, -6.8062e-01,  1.8555e-01,  1.1641e-01,\n",
            "          4.5058e-01, -1.4022e-02, -2.3160e-02,  4.4142e-02, -8.6119e-02]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "outputs shape:  torch.Size([64, 10])\n",
            "Sample outputs: \n",
            " tensor([[-0.1530,  0.0387, -0.0134, -0.1738, -0.0815,  0.3285, -0.1818,  0.0122,\n",
            "          0.1097,  0.0870],\n",
            "        [-0.4713,  0.6098, -0.8029, -0.2247,  0.3991, -0.0051, -0.9512, -0.2933,\n",
            "          0.2634, -0.1703]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(outputs, dim = 1)\n",
        "\n",
        "## chaecking at sample probabilities\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "print(\"\\n\")\n",
        "## Add up the probabilities of an output row\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())\n",
        "max_probs, preds = torch.max(probs, dim = 1)\n",
        "print(\"\\n\")\n",
        "print(preds)\n",
        "print(\"\\n\")\n",
        "print(max_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu9fP9dFFtAu",
        "outputId": "b9ec9eff-f440-475c-f251-ad3558ef8cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample probabilities:\n",
            " tensor([[0.0851, 0.1030, 0.0978, 0.0833, 0.0914, 0.1377, 0.0826, 0.1003, 0.1106,\n",
            "         0.1081],\n",
            "        [0.0659, 0.1942, 0.0473, 0.0843, 0.1573, 0.1050, 0.0408, 0.0787, 0.1374,\n",
            "         0.0890]])\n",
            "\n",
            "\n",
            "Sum:  1.0\n",
            "\n",
            "\n",
            "tensor([5, 1, 4, 1, 5, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 5, 1,\n",
            "        1, 4, 7, 4, 1, 8, 1, 1, 5, 1, 1, 1, 5, 4, 8, 1, 1, 5, 1, 1, 4, 1, 5, 1,\n",
            "        5, 1, 8, 1, 8, 4, 1, 1, 1, 4, 1, 1, 5, 4, 1, 5])\n",
            "\n",
            "\n",
            "tensor([0.1377, 0.1942, 0.1531, 0.1926, 0.1360, 0.1625, 0.1146, 0.1303, 0.1580,\n",
            "        0.1894, 0.1889, 0.1652, 0.1306, 0.1769, 0.1900, 0.1510, 0.1599, 0.1512,\n",
            "        0.1389, 0.2430, 0.1269, 0.1304, 0.1504, 0.1725, 0.1349, 0.1705, 0.1281,\n",
            "        0.1848, 0.1611, 0.1382, 0.1738, 0.1755, 0.1478, 0.1666, 0.1392, 0.2149,\n",
            "        0.1569, 0.1331, 0.1574, 0.1834, 0.1964, 0.1113, 0.1829, 0.2230, 0.1554,\n",
            "        0.1541, 0.1287, 0.1606, 0.1376, 0.1235, 0.1134, 0.1303, 0.1341, 0.1293,\n",
            "        0.1363, 0.1912, 0.1838, 0.1591, 0.1359, 0.1982, 0.1312, 0.1498, 0.2122,\n",
            "        0.1505], grad_fn=<MaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric and Loss Function¶\n",
        "Here we evaluate our model by finding the percentage of labels that were predicted correctly i.e. the accuracy of the predictions.\n",
        "\n",
        "The == performas an element-wise comparision of two tensors with the same shape, and returns a tensor of the same shape,containing 0s for unequal elements, and 1s for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally we divide by the total total number of images to get the accuracy."
      ],
      "metadata": {
        "id": "vUd3hpZQIdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim = 1)\n",
        "    return(torch.tensor(torch.sum(preds == labels).item()/ len(preds)))\n",
        "\n",
        "print(\"Accuracy: \",accuracy(outputs, labels))\n",
        "print(\"\\n\")\n",
        "loss_fn = F.cross_entropy\n",
        "print(\"Loss Function: \",loss_fn)\n",
        "print(\"\\n\")\n",
        "## Loss for the current batch\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuIYAHN0IeBO",
        "outputId": "2e0b6da4-0ea7-42ca-d9ee-9d602261322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  tensor(0.0938)\n",
            "\n",
            "\n",
            "Loss Function:  <function cross_entropy at 0x7f8e943293a0>\n",
            "\n",
            "\n",
            "tensor(2.4179, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    }
  ]
}